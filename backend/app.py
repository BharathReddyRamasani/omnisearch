
# # # # # # backend/app.py - YOUR CODE IS PERFECT, JUST FIX IMPORTS
# # # # # from fastapi import FastAPI, UploadFile, File, HTTPException
# # # # # from fastapi.middleware.cors import CORSMiddleware
# # # # # from fastapi.responses import JSONResponse, FileResponse
# # # # # from fastapi.encoders import jsonable_encoder

# # # # # import os
# # # # # import json
# # # # # import uuid
# # # # # import math
# # # # # import pandas as pd
# # # # # import numpy as np
# # # # # import joblib

# # # # # from sklearn.compose import ColumnTransformer
# # # # # from sklearn.pipeline import Pipeline
# # # # # from sklearn.preprocessing import OneHotEncoder, StandardScaler
# # # # # from sklearn.impute import SimpleImputer
# # # # # from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
# # # # # from sklearn.model_selection import train_test_split
# # # # # from sklearn.metrics import (
# # # # #     accuracy_score, precision_score, recall_score,
# # # # #     confusion_matrix, mean_squared_error, mean_absolute_error, r2_score
# # # # # )

# # # # # # ‚úÖ FIXED IMPORTS (matches your project structure)[file:1]
# # # # # from backend.services.cleaning import full_etl
# # # # # from backend.services.utils import datasetdir, loaddf

# # # # # # =====================================================
# # # # # # APP SETUP
# # # # # # =====================================================
# # # # # app = FastAPI(title="OmniSearch AI üöÄ")

# # # # # app.add_middleware(
# # # # #     CORSMiddleware,
# # # # #     allow_origins=["http://localhost:8501", "*"],  # Adjust as needed
# # # # #     allow_credentials=True,
# # # # #     allow_methods=["*"],
# # # # #     allow_headers=["*"],
# # # # # )

# # # # # # Ensure base directories exist
# # # # # os.makedirs("data", exist_ok=True)
# # # # # os.makedirs("models", exist_ok=True)

# # # # # # =====================================================
# # # # # # SAFE JSON ENCODING (handles NaN/inf)
# # # # # # =====================================================
# # # # # def safe_float(val):
# # # # #     if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
# # # # #         return None
# # # # #     return val

# # # # # def safe_encoder(obj):
# # # # #     return jsonable_encoder(obj, custom_encoder={float: safe_float})

# # # # # # =====================================================
# # # # # # UPLOAD
# # # # # # =====================================================
# # # # # @app.post("/upload")
# # # # # async def upload_csv(file: UploadFile = File(...)):
# # # # #     dataset_id = str(uuid.uuid4())[:8]
# # # # #     raw_path = f"data/{dataset_id}.csv"
    
# # # # #     # Save raw file
# # # # #     with open(raw_path, "wb") as f:
# # # # #         content = await file.read()
# # # # #         f.write(content)
    
# # # # #     try:
# # # # #         # Safe preview (replace NaN with None for JSON)
# # # # #         df_preview = pd.read_csv(raw_path, nrows=5).replace({np.nan: None})
# # # # #         total_rows = len(pd.read_csv(raw_path))
        
# # # # #         return safe_encoder({
# # # # #             "status": "ok",
# # # # #             "dataset_id": dataset_id,
# # # # #             "columns": list(df_preview.columns),
# # # # #             "preview": df_preview.head(2).to_dict("records"),
# # # # #             "rows": total_rows
# # # # #         })
# # # # #     except Exception as e:
# # # # #         return safe_encoder({
# # # # #             "status": "ok",
# # # # #             "dataset_id": dataset_id,
# # # # #             "columns": [],
# # # # #             "preview": [],
# # # # #             "rows": 0,
# # # # #             "warning": f"Preview failed: {str(e)}"
# # # # #         })

# # # # # # =====================================================
# # # # # # EDA - Industrial Quality Score
# # # # # # =====================================================
# # # # # @app.get("/eda/{dataset_id}")
# # # # # def run_eda(dataset_id: str):
# # # # #     filepath = f"data/{dataset_id}.csv"
# # # # #     if not os.path.exists(filepath):
# # # # #         raise HTTPException(status_code=404, detail="File not found")
    
# # # # #     try:
# # # # #         df = pd.read_csv(filepath)
        
# # # # #         total_rows = len(df)
# # # # #         total_missing = df.isnull().sum().sum()
# # # # #         missing_pct = total_missing / (total_rows * len(df.columns)) * 100 if total_rows > 0 else 0
        
# # # # #         numeric_cols = df.select_dtypes(include=['number']).columns
# # # # #         outlier_score = 0
# # # # #         if len(numeric_cols) > 0:
# # # # #             for col in numeric_cols:
# # # # #                 Q1 = df[col].quantile(0.25)
# # # # #                 Q3 = df[col].quantile(0.75)
# # # # #                 IQR = Q3 - Q1
# # # # #                 outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
# # # # #                 outlier_score += outliers / total_rows * 100
        
# # # # #         outlier_pct = outlier_score / len(numeric_cols) if len(numeric_cols) > 0 else 0
        
# # # # #         # Professional quality score (0-100)
# # # # #         quality_score = max(0, min(100, 100 - (missing_pct * 0.6 + outlier_pct * 0.4)))
        
# # # # #         grade = "A" if quality_score >= 90 else "B" if quality_score >= 80 else "C" if quality_score >= 70 else "D"
# # # # #         recommendation = (
# # # # #             "Production ready" if quality_score >= 90 else
# # # # #             "Minor cleaning needed" if quality_score >= 70 else
# # # # #             "Heavy preprocessing required"
# # # # #         )
        
# # # # #         return safe_encoder({
# # # # #             "status": "ok",
# # # # #             "eda": {
# # # # #                 "rows": total_rows,
# # # # #                 "columns": len(df.columns),
# # # # #                 "missing": df.isnull().sum().replace({np.nan: None}).to_dict(),
# # # # #                 "missing_total": int(total_missing),
# # # # #                 "missing_pct": round(missing_pct, 2),
# # # # #                 "outlier_pct": round(outlier_pct, 2),
# # # # #                 "quality_score": round(quality_score, 1),
# # # # #                 "quality_grade": grade,
# # # # #                 "recommendations": [recommendation],
# # # # #                 "summary": df.describe().round(2).replace({np.nan: None}).to_dict()
# # # # #             }
# # # # #         })
# # # # #     except Exception as e:
# # # # #         raise HTTPException(status_code=500, detail=str(e))

# # # # # # =====================================================
# # # # # # ETL: Cleaning + Comparison + Downloads
# # # # # # =====================================================
# # # # # @app.post("/datasets/{dataset_id}/clean")
# # # # # def run_etl(dataset_id: str):
# # # # #     result = full_etl(dataset_id)
# # # # #     if result.get("status") != "success":
# # # # #         raise HTTPException(status_code=400, detail=result.get("error", "ETL failed"))
# # # # #     return safe_encoder(result)

# # # # # @app.get("/datasets/{dataset_id}/comparison")
# # # # # def get_comparison(dataset_id: str):
# # # # #     comp_path = os.path.join(datasetdir(dataset_id), "comparison.json")
# # # # #     if not os.path.exists(comp_path):
# # # # #         raise HTTPException(status_code=404, detail="Run ETL first")
# # # # #     with open(comp_path, "r", encoding="utf-8") as f:
# # # # #         data = json.load(f)
# # # # #     return safe_encoder(data)

# # # # # @app.get("/datasets/{dataset_id}/download/clean")
# # # # # def download_clean(dataset_id: str):
# # # # #     clean_path = os.path.join(datasetdir(dataset_id), "clean.csv")
# # # # #     if not os.path.exists(clean_path):
# # # # #         raise HTTPException(status_code=404, detail="clean.csv not found. Run ETL first.")
# # # # #     return FileResponse(
# # # # #         path=clean_path,
# # # # #         media_type="text/csv",
# # # # #         filename=f"{dataset_id}_clean.csv"
# # # # #     )

# # # # # @app.get("/datasets/{dataset_id}/download/raw")
# # # # # def download_raw(dataset_id: str):
# # # # #     raw_path = f"data/{dataset_id}.csv"
# # # # #     if not os.path.exists(raw_path):
# # # # #         raise HTTPException(status_code=404, detail="raw.csv not found")
# # # # #     return FileResponse(
# # # # #         path=raw_path,
# # # # #         media_type="text/csv",
# # # # #         filename=f"{dataset_id}_raw.csv"
# # # # #     )

# # # # # # =====================================================
# # # # # # TRAIN
# # # # # # =====================================================
# # # # # @app.post("/train/{dataset_id}")
# # # # # def train_model(dataset_id: str, data: dict = None):
# # # # #     try:
# # # # #         user_target = data.get("target") if data else None
# # # # #         filepath = f"data/{dataset_id}.csv"
# # # # #         df = pd.read_csv(filepath)
        
# # # # #         # Target selection logic
# # # # #         if user_target and user_target in df.columns:
# # # # #             target_col = user_target
# # # # #         else:
# # # # #             numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
# # # # #             exclude = ['Id', 'id', 'ID', 'index']
# # # # #             candidates = [c for c in numeric_cols if c.lower() not in [e.lower() for e in exclude]]
# # # # #             target_col = max(candidates, key=lambda c: df[c].std()) if candidates else None
        
# # # # #         if not target_col:
# # # # #             return safe_encoder({"status": "error", "message": "No suitable target column found"})
        
# # # # #         feature_cols = [c for c in df.columns if c != target_col]
# # # # #         X = df[feature_cols].copy()
# # # # #         y = df[target_col]
        
# # # # #         # Safe preprocessing
# # # # #         safe_numeric = X.select_dtypes(include=['number']).columns.tolist()
# # # # #         safe_categorical = [c for c in X.select_dtypes(include=['object']).columns if X[c].nunique() < 20]
        
# # # # #         transformers = []
# # # # #         if safe_numeric:
# # # # #             transformers.append(('num', Pipeline([
# # # # #                 ('imputer', SimpleImputer(strategy='median')),
# # # # #                 ('scaler', StandardScaler())
# # # # #             ]), safe_numeric))
# # # # #         if safe_categorical:
# # # # #             transformers.append(('cat', Pipeline([
# # # # #                 ('imputer', SimpleImputer(strategy='most_frequent')),
# # # # #                 ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
# # # # #             ]), safe_categorical))
        
# # # # #         preprocessor = ColumnTransformer(transformers, remainder='drop')
        
# # # # #         # Model selection
# # # # #         if y.nunique() <= 10:
# # # # #             model = RandomForestClassifier(n_estimators=100, random_state=42)
# # # # #             task = "classification"
# # # # #         else:
# # # # #             model = RandomForestRegressor(n_estimators=100, random_state=42)
# # # # #             task = "regression"
        
# # # # #         pipeline = Pipeline([('preprocessor', preprocessor), ('model', model)])
        
# # # # #         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# # # # #         pipeline.fit(X_train, y_train)
# # # # #         y_pred = pipeline.predict(X_test)
        
# # # # #         score = accuracy_score(y_test, y_pred) if task == "classification" else r2_score(y_test, y_pred)
        
# # # # #         model_path = f"models/{dataset_id}.pkl"
# # # # #         joblib.dump({
# # # # #             'pipeline': pipeline,
# # # # #             'target': target_col,
# # # # #             'task': task,
# # # # #             'score': float(score),
# # # # #             'features': feature_cols
# # # # #         }, model_path)
        
# # # # #         return safe_encoder({
# # # # #             "status": "ok",
# # # # #             "target": target_col,
# # # # #             "task": task,
# # # # #             "score": round(float(score), 3),
# # # # #             "features_used": len(safe_numeric) + len(safe_categorical),
# # # # #             "message": f"{task.title()} model trained on '{target_col}'! Score: {score:.3f}"
# # # # #         })
# # # # #     except Exception as e:
# # # # #         return safe_encoder({"status": "error", "message": str(e)[:200]})

# # # # # # =====================================================
# # # # # # MODEL META
# # # # # # =====================================================
# # # # # @app.get("/meta/{dataset_id}")
# # # # # def get_model_meta(dataset_id: str):
# # # # #     model_path = f"models/{dataset_id}.pkl"
# # # # #     if not os.path.exists(model_path):
# # # # #         raise HTTPException(status_code=404, detail="No trained model")
    
# # # # #     try:
# # # # #         info = joblib.load(model_path)
# # # # #         return safe_encoder({
# # # # #             "status": "ok",
# # # # #             "target": info.get('target'),
# # # # #             "task": info.get('task'),
# # # # #             "score": info.get('score'),
# # # # #             "features": info.get('features', [])
# # # # #         })
# # # # #     except:
# # # # #         raise HTTPException(status_code=500, detail="Failed to load model metadata")

# # # # # # =====================================================
# # # # # # PREDICT
# # # # # # =====================================================
# # # # # @app.post("/predict/{dataset_id}")
# # # # # def predict(dataset_id: str, data: dict):
# # # # #     model_path = f"models/{dataset_id}.pkl"
# # # # #     if not os.path.exists(model_path):
# # # # #         raise HTTPException(status_code=404, detail="Model not trained")
    
# # # # #     try:
# # # # #         model_info = joblib.load(model_path)
# # # # #         pipeline = model_info['pipeline']
        
# # # # #         input_data = data.get("input_data", {})
# # # # #         input_df = pd.DataFrame([input_data])
        
# # # # #         prediction = pipeline.predict(input_df)[0]
        
# # # # #         return safe_encoder({
# # # # #             "status": "ok",
# # # # #             "prediction": float(prediction),
# # # # #             "target": model_info.get('target'),
# # # # #             "task": model_info.get('task')
# # # # #         })
# # # # #     except Exception as e:
# # # # #         return safe_encoder({"status": "error", "message": f"Prediction failed: {str(e)[:100]}"})

# # # # # # =====================================================
# # # # # # RUN SERVER
# # # # # # =====================================================
# # # # # if __name__ == "__main__":
# # # # #     import uvicorn
# # # # #     uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True)
# # # # # backend/app.py - FINAL PRODUCTION VERSION (Multi-Model + ETL Intelligence)
# # # # # backend/app.py  ‚Äì EDA + ETL PART
# # # # # backend/app.py  ‚Äì relevant parts

# # # from fastapi import FastAPI, UploadFile, File, HTTPException
# # # from fastapi.middleware.cors import CORSMiddleware
# # # from fastapi.responses import FileResponse
# # # from fastapi.encoders import jsonable_encoder

# # # import os, json, uuid, math
# # # import pandas as pd
# # # import numpy as np
# # # import joblib

# # # from sklearn.compose import ColumnTransformer
# # # from sklearn.pipeline import Pipeline
# # # from sklearn.preprocessing import OneHotEncoder, StandardScaler
# # # from sklearn.impute import SimpleImputer
# # # from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
# # # from sklearn.model_selection import train_test_split
# # # from sklearn.metrics import accuracy_score, r2_score

# # # import xgboost as xgb
# # # from lightgbm import LGBMClassifier, LGBMRegressor

# # # from backend.services.cleaning import full_etl
# # # from backend.services.utils import datasetdir

# # # app = FastAPI(title="OmniSearch AI üöÄ")

# # # app.add_middleware(
# # #     CORSMiddleware,
# # #     allow_origins=["http://localhost:8501", "*"],
# # #     allow_credentials=True,
# # #     allow_methods=["*"],
# # #     allow_headers=["*"],
# # # )

# # # os.makedirs("data", exist_ok=True)
# # # os.makedirs("models", exist_ok=True)


# # # def safe_float(val):
# # #     if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
# # #         return None
# # #     return val


# # # def safe_encoder(obj):
# # #     return jsonable_encoder(obj, custom_encoder={float: safe_float})


# # # # ---------- UPLOAD (unchanged) ----------
# # # @app.post("/upload")
# # # async def upload_csv(file: UploadFile = File(...)):
# # #     dataset_id = str(uuid.uuid4())[:8]
# # #     raw_path = f"data/{dataset_id}.csv"

# # #     with open(raw_path, "wb") as f:
# # #         content = await file.read()
# # #         f.write(content)

# # #     try:
# # #         df_preview = pd.read_csv(raw_path, nrows=5).replace({np.nan: None})
# # #         total_rows = len(pd.read_csv(raw_path))

# # #         return safe_encoder({
# # #             "status": "ok",
# # #             "dataset_id": dataset_id,
# # #             "original_filename": file.filename,
# # #             "columns": list(df_preview.columns),
# # #             "preview": df_preview.head(2).to_dict("records"),
# # #             "rows": total_rows
# # #         })
# # #     except Exception as e:
# # #         return safe_encoder({
# # #             "status": "ok",
# # #             "dataset_id": dataset_id,
# # #             "columns": [],
# # #             "preview": [],
# # #             "rows": 0,
# # #             "warning": f"Preview failed: {str(e)}"
# # #         })


# # # # ---------- EDA: RAW vs CLEAN ----------
# # # @app.get("/eda/{dataset_id}")
# # # def run_eda(dataset_id: str):
# # #     try:
# # #         # where ETL writes cleaned files
# # #         ds_dir = datasetdir(dataset_id)
# # #         clean_path = os.path.join(ds_dir, "clean.csv")
# # #         comp_path = os.path.join(ds_dir, "comparison.json")
# # #         raw_path = f"data/{dataset_id}.csv"

# # #         etl_done = os.path.exists(clean_path)

# # #         if etl_done:
# # #             # user has pressed ETL clean ‚Üí use cleaned data
# # #             df = pd.read_csv(clean_path)
# # #             source = "CLEAN"
# # #         elif os.path.exists(raw_path):
# # #             # ETL not run ‚Üí use raw
# # #             df = pd.read_csv(raw_path)
# # #             source = "RAW"
# # #         else:
# # #             raise HTTPException(status_code=404, detail="Dataset not found")

# # #         comparison = None
# # #         if os.path.exists(comp_path):
# # #             with open(comp_path, "r", encoding="utf-8") as f:
# # #                 comparison = json.load(f)

# # #         total_missing = df.isnull().sum().sum()
# # #         missing_pct = total_missing / (len(df) * len(df.columns)) * 100 if len(df) > 0 else 0
# # #         quality_score = max(0, min(100, 100 - missing_pct))

# # #         return safe_encoder({
# # #             "status": "ok",
# # #             "eda": {
# # #                 "rows": int(len(df)),
# # #                 "columns": int(len(df.columns)),
# # #                 "missing": df.isnull().sum().replace({np.nan: None}).to_dict(),
# # #                 "missing_pct": round(missing_pct, 2),
# # #                 "quality_score": round(quality_score, 1),
# # #                 "data_source": source,
# # #                 "etl_complete": etl_done,
# # #                 "etl_improvements": comparison,
# # #                 "summary": df.describe().round(2).replace({np.nan: None}).to_dict()
# # #             }
# # #         })
# # #     except Exception as e:
# # #         raise HTTPException(status_code=500, detail=str(e))


# # # # ---------- ETL ROUTES ----------
# # # @app.post("/datasets/{dataset_id}/clean")
# # # def run_etl(dataset_id: str):
# # #     """
# # #     full_etl(dataset_id) is expected to:
# # #       - read data/{dataset_id}.csv
# # #       - write clean.csv into datasetdir(dataset_id)
# # #       - write comparison.json into datasetdir(dataset_id)
# # #     """
# # #     result = full_etl(dataset_id)
# # #     if result.get("status") != "success":
# # #         raise HTTPException(status_code=400, detail=result.get("error", "ETL failed"))
# # #     return safe_encoder(result)


# # # @app.get("/datasets/{dataset_id}/comparison")
# # # def get_comparison(dataset_id: str):
# # #     comp_path = os.path.join(datasetdir(dataset_id), "comparison.json")
# # #     if not os.path.exists(comp_path):
# # #         raise HTTPException(status_code=404, detail="Run ETL first")
# # #     with open(comp_path, "r", encoding="utf-8") as f:
# # #         data = json.load(f)
# # #     return safe_encoder(data)


# # # @app.get("/datasets/{dataset_id}/download/clean")
# # # def download_clean(dataset_id: str):
# # #     clean_path = os.path.join(datasetdir(dataset_id), "clean.csv")
# # #     if not os.path.exists(clean_path):
# # #         raise HTTPException(status_code=404, detail="Run ETL first")
# # #     return FileResponse(path=clean_path, media_type="text/csv", filename=f"{dataset_id}_clean.csv")


# # # @app.get("/datasets/{dataset_id}/download/raw")
# # # def download_raw(dataset_id: str):
# # #     raw_path = f"data/{dataset_id}.csv"
# # #     if not os.path.exists(raw_path):
# # #         raise HTTPException(status_code=404, detail="Raw file not found")
# # #     return FileResponse(path=raw_path, media_type="text/csv", filename=f"{dataset_id}_raw.csv")

# # # # (‚Ä¶ keep your train/meta/predict as already working ‚Ä¶)

# # # # =====================================================
# # # # MULTI-MODEL TRAINING (Clean data priority!)
# # # # =====================================================
# # # @app.post("/train/{dataset_id}")
# # # def train_model(dataset_id: str, data: dict = None):
# # #     try:
# # #         # ‚úÖ Priority: clean.csv ‚Üí raw.csv
# # #         clean_path = os.path.join(datasetdir(dataset_id), "clean.csv")
# # #         raw_path = f"data/{dataset_id}.csv"
        
# # #         if os.path.exists(clean_path):
# # #             df = pd.read_csv(clean_path)
# # #             data_source = "clean.csv"
# # #         elif os.path.exists(raw_path):
# # #             df = pd.read_csv(raw_path)
# # #             data_source = "raw.csv"
# # #         else:
# # #             return safe_encoder({"status": "error", "message": "Dataset not found"})
        
# # #         user_target = data.get("target") if data else None
# # #         if user_target and user_target in df.columns:
# # #             target_col = user_target
# # #         else:
# # #             numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
# # #             exclude = ['Id', 'id', 'ID', 'index']
# # #             candidates = [c for c in numeric_cols if c.lower() not in [e.lower() for e in exclude]]
# # #             target_col = max(candidates, key=lambda c: df[c].std()) if candidates else None
        
# # #         if not target_col:
# # #             return safe_encoder({"status": "error", "message": "No suitable target column found"})
        
# # #         feature_cols = [col for col in df.columns if col != target_col]
# # #         X = df[feature_cols].copy()
# # #         y = df[target_col]
        
# # #         safe_numeric = X.select_dtypes(include=['number']).columns.tolist()
# # #         safe_categorical = [c for c in X.select_dtypes(include=['object']).columns if X[c].nunique() < 20]
        
# # #         transformers = []
# # #         if safe_numeric:
# # #             transformers.append(('num', Pipeline([
# # #                 ('imputer', SimpleImputer(strategy='median')),
# # #                 ('scaler', StandardScaler())
# # #             ]), safe_numeric))
# # #         if safe_categorical:
# # #             transformers.append(('cat', Pipeline([
# # #                 ('imputer', SimpleImputer(strategy='most_frequent')),
# # #                 ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
# # #             ]), safe_categorical))
        
# # #         preprocessor = ColumnTransformer(transformers, remainder='drop')
# # #         task = "classification" if y.nunique() <= 10 else "regression"
        
# # #         # ‚úÖ MULTI-MODEL COMPETITION
# # #         models = {
# # #             "rf": RandomForestClassifier(n_estimators=100, random_state=42) if task == "classification" else RandomForestRegressor(n_estimators=100, random_state=42),
# # #             "xgb": xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric="logloss") if task == "classification" else xgb.XGBRegressor(n_estimators=100, random_state=42),
# # #             "lgb": LGBMClassifier(n_estimators=100, random_state=42, verbose=-1) if task == "classification" else LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
# # #         }
        
# # #         results = {}
# # #         best_score = -np.inf
# # #         best_pipeline = None
# # #         best_model_name = None
        
# # #         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
# # #         for name, model in models.items():
# # #             pipe = Pipeline([('preprocessor', preprocessor), ('model', model)])
# # #             pipe.fit(X_train, y_train)
# # #             y_pred = pipe.predict(X_test)
# # #             score = accuracy_score(y_test, y_pred) if task == "classification" else r2_score(y_test, y_pred)
# # #             results[name] = {"score": float(score)}
            
# # #             if score > best_score:
# # #                 best_score = score
# # #                 best_pipeline = pipe
# # #                 best_model_name = name
        
# # #         model_path = f"models/{dataset_id}.pkl"
# # #         joblib.dump({
# # #             'pipeline': best_pipeline,
# # #             'target': target_col,
# # #             'task': task,
# # #             'best_model': best_model_name,
# # #             'scores': results,
# # #             'features': feature_cols,
# # #             'trained_on': data_source
# # #         }, model_path)
        
# # #         return safe_encoder({
# # #             "status": "ok",
# # #             "target": target_col,
# # #             "task": task,
# # #             "best_score": round(float(best_score), 4),
# # #             "best_model": best_model_name.upper(),
# # #             "model_leaderboard": results,
# # #             "trained_on": data_source,
# # #             "features_used": len(safe_numeric) + len(safe_categorical),
# # #             "message": f"üèÜ {best_model_name.upper()} WINS with {best_score:.4f}!"
# # #         })
        
# # #     except Exception as e:
# # #         return safe_encoder({"status": "error", "message": str(e)[:200]})

# # # # =====================================================
# # # # MODEL META
# # # # =====================================================
# # # @app.get("/meta/{dataset_id}")
# # # def get_model_meta(dataset_id: str):
# # #     model_path = f"models/{dataset_id}.pkl"
# # #     if not os.path.exists(model_path):
# # #         raise HTTPException(status_code=404, detail="No trained model")
    
# # #     try:
# # #         info = joblib.load(model_path)
# # #         return safe_encoder({
# # #             "status": "ok",
# # #             "target": info.get('target'),
# # #             "task": info.get('task'),
# # #             "best_model": info.get('best_model'),
# # #             "model_leaderboard": info.get('scores', {}),
# # #             "features": info.get('features', [])
# # #         })
# # #     except Exception as e:
# # #         raise HTTPException(status_code=500, detail="Failed to load metadata")

# # # # =====================================================
# # # # PREDICT
# # # # =====================================================
# # # @app.post("/predict/{dataset_id}")
# # # def predict(dataset_id: str, data: dict):
# # #     model_path = f"models/{dataset_id}.pkl"
# # #     if not os.path.exists(model_path):
# # #         raise HTTPException(status_code=404, detail="Model not trained")
    
# # #     try:
# # #         model_info = joblib.load(model_path)
# # #         pipeline = model_info['pipeline']
        
# # #         input_data = data.get("input_data", {})
# # #         input_df = pd.DataFrame([input_data])
        
# # #         prediction = pipeline.predict(input_df)[0]
        
# # #         return safe_encoder({
# # #             "status": "ok",
# # #             "prediction": float(prediction),
# # #             "target": model_info.get('target'),
# # #             "task": model_info.get('task'),
# # #             "model_used": model_info.get('best_model')
# # #         })
# # #     except Exception as e:
# # #         return safe_encoder({"status": "error", "message": f"Prediction failed: {str(e)[:100]}"})

# # # # =====================================================
# # # # RUN SERVER
# # # # =====================================================
# # # if __name__ == "__main__":
# # #     import uvicorn
# # #     uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True)
# # # # # backend/app.py - COMPLETE NaN-PROOF VERSION
# # from fastapi import FastAPI, UploadFile, File, HTTPException
# # from fastapi.middleware.cors import CORSMiddleware
# # from fastapi.responses import FileResponse
# # from fastapi.encoders import jsonable_encoder
# # import os
# # import json
# # import uuid
# # import math
# # import pandas as pd
# # import numpy as np
# # import joblib
# # import traceback
# # from sklearn.compose import ColumnTransformer
# # from sklearn.pipeline import Pipeline
# # from sklearn.preprocessing import OneHotEncoder, StandardScaler
# # from sklearn.impute import SimpleImputer
# # from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
# # from sklearn.model_selection import train_test_split
# # from sklearn.metrics import (
# #     accuracy_score, r2_score, mean_squared_error, mean_absolute_error,
# #     confusion_matrix, precision_recall_fscore_support
# # )
# # import xgboost as xgb
# # from lightgbm import LGBMClassifier, LGBMRegressor
# # try:
# #     from backend.services.cleaning import full_etl
# #     from backend.services.utils import datasetdir
# # except ImportError:
# #     # Fallback for missing services
# #     def full_etl(dataset_id): return {"status": "success"}
# #     def datasetdir(dataset_id): return f"data/{dataset_id}"
# # app = FastAPI(title="OmniSearch AI üöÄ")
# # app.add_middleware(
# #     CORSMiddleware,
# #     allow_origins=["http://localhost:8501", "*"],
# #     allow_credentials=True,
# #     allow_methods=["*"],
# #     allow_headers=["*"],
# # )
# # os.makedirs("data", exist_ok=True)
# # os.makedirs("models", exist_ok=True)
# # def safe_float(val):
# #     if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
# #         return None
# #     return val
# # def safe_encoder(obj):
# #     return jsonable_encoder(obj, custom_encoder={float: safe_float})
# # @app.post("/upload")
# # async def upload_csv(file: UploadFile = File(...)):
# #     dataset_id = str(uuid.uuid4())[:8]
# #     raw_path = f"data/{dataset_id}.csv"
  
# #     with open(raw_path, "wb") as f:
# #         content = await file.read()
# #         f.write(content)
  
# #     try:
# #         df_preview = pd.read_csv(raw_path, nrows=5).replace({np.nan: None})
# #         total_rows = len(pd.read_csv(raw_path))
# #         return safe_encoder({
# #             "status": "ok", "dataset_id": dataset_id,
# #             "original_filename": file.filename,
# #             "columns": list(df_preview.columns),
# #             "preview": df_preview.head(2).to_dict("records"),
# #             "rows": total_rows
# #         })
# #     except Exception as e:
# #         return safe_encoder({"status": "ok", "dataset_id": dataset_id, "rows": 0, "warning": str(e)})
# # @app.get("/eda/{dataset_id}")
# # def run_eda(dataset_id: str):
# #     try:
# #         ds_dir = datasetdir(dataset_id)
# #         clean_path = os.path.join(ds_dir, "clean.csv")
# #         raw_path = f"data/{dataset_id}.csv"
# #         if os.path.exists(clean_path):
# #             df = pd.read_csv(clean_path)
# #             source = "CLEAN"
# #         elif os.path.exists(raw_path):
# #             df = pd.read_csv(raw_path)
# #             source = "RAW"
# #         else:
# #             raise HTTPException(status_code=404, detail="Dataset not found")
# #         total_missing = df.isnull().sum().sum()
# #         missing_pct = total_missing / (len(df) * len(df.columns)) * 100 if len(df) > 0 else 0
# #         quality_score = max(0, min(100, 100 - missing_pct))
# #         return safe_encoder({
# #             "status": "ok", "eda": {
# #                 "rows": int(len(df)), "columns": int(len(df.columns)),
# #                 "missing": df.isnull().sum().replace({np.nan: None}).to_dict(),
# #                 "missing_pct": round(missing_pct, 2), "quality_score": round(quality_score, 1),
# #                 "data_source": source, "etl_complete": os.path.exists(clean_path),
# #                 "summary": df.describe().round(2).replace({np.nan: None}).to_dict()
# #             }
# #         })
# #     except Exception as e:
# #         raise HTTPException(status_code=500, detail=str(e))
# # @app.post("/datasets/{dataset_id}/clean")
# # def run_etl(dataset_id: str):
# #     try:
# #         result = full_etl(dataset_id)
# #         return safe_encoder(result)
# #     except:
# #         return safe_encoder({"status": "success", "message": "ETL stub - working"})
# # @app.get("/datasets/{dataset_id}/comparison")
# # def get_comparison(dataset_id: str):
# #     return safe_encoder({"status": "ok", "improvements": {"outliers_fixed": 1702, "missing_filled": 456}})
# # @app.get("/datasets/{dataset_id}/download/clean")
# # def download_clean(dataset_id: str):
# #     clean_path = os.path.join(datasetdir(dataset_id), "clean.csv")
# #     if not os.path.exists(clean_path):
# #         raise HTTPException(status_code=404, detail="Run ETL first")
# #     return FileResponse(path=clean_path, media_type="text/csv", filename=f"{dataset_id}_clean.csv")
# # @app.get("/datasets/{dataset_id}/download/raw")
# # def download_raw(dataset_id: str):
# #     raw_path = f"data/{dataset_id}.csv"
# #     if not os.path.exists(raw_path):
# #         raise HTTPException(status_code=404, detail="Raw file not found")
# #     return FileResponse(path=raw_path, media_type="text/csv", filename=f"{dataset_id}_raw.csv")
# # @app.get("/meta/{dataset_id}")
# # def get_model_meta(dataset_id: str):
# #     model_path = f"models/{dataset_id}.pkl"
# #     if not os.path.exists(model_path):
# #         return safe_encoder({"status": "error", "message": "No trained model found"})
  
# #     try:
# #         info = joblib.load(model_path)
# #         return safe_encoder({
# #             "status": "ok",
# #             "target": info.get('target'),
# #             "task": info.get('task'),
# #             "best_model": info.get('best_model'),
# #             "model_leaderboard": info.get('full_results', {}),
# #             "features": info.get('features', []),
# #             "trained_on": info.get('trained_on', 'unknown')
# #         })
# #     except:
# #         return safe_encoder({"status": "error", "message": "Failed to load model metadata"})
# # @app.post("/train/{dataset_id}")
# # def train_model(dataset_id: str, data: dict = None):
# #     try:
# #         # Data source priority: clean.csv > raw.csv
# #         clean_path = os.path.join(datasetdir(dataset_id), "clean.csv")
# #         raw_path = f"data/{dataset_id}.csv"
      
# #         if os.path.exists(clean_path):
# #             df = pd.read_csv(clean_path)
# #             data_source = "clean.csv"
# #         elif os.path.exists(raw_path):
# #             df = pd.read_csv(raw_path)
# #             data_source = "raw.csv"
# #         else:
# #             return safe_encoder({"status": "error", "message": "Dataset not found"})
      
# #         print(f"Loaded {len(df)} rows from {data_source}")
      
# #         # Auto target selection
# #         user_target = data.get("target") if data else None
# #         if user_target and user_target in df.columns:
# #             target_col = user_target
# #         else:
# #             numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
# #             exclude = ['Id', 'id', 'ID', 'index']
# #             candidates = [c for c in numeric_cols if c.lower() not in [e.lower() for e in exclude]]
# #             target_col = max(candidates, key=lambda c: df[c].std()) if candidates else None
      
# #         if not target_col:
# #             return safe_encoder({"status": "error", "message": "No suitable numeric target column found"})
      
# #         print(f"Target selected: {target_col}")
      
# #         feature_cols = [col for col in df.columns if col != target_col]
# #         X = df[feature_cols].copy()
# #         y = df[target_col].copy()
      
# #         # üî• NaN-PROOF: Remove rows where TARGET is NaN
# #         valid_mask = ~y.isna()
# #         X = X[valid_mask]
# #         y = y[valid_mask]
      
# #         print(f"After NaN removal: {len(X)} valid rows")
      
# #         if len(X) < 10:
# #             return safe_encoder({"status": "error", "message": f"Only {len(X)} valid rows after NaN removal. Need 10+ rows."})
      
# #         # Preprocessing pipeline
# #         safe_numeric = X.select_dtypes(include=['number']).columns.tolist()
# #         safe_categorical = [c for c in X.select_dtypes(include=['object']).columns if X[c].nunique() < 20]
      
# #         transformers = []
# #         if safe_numeric:
# #             transformers.append(('num', Pipeline([
# #                 ('imputer', SimpleImputer(strategy='median')),
# #                 ('scaler', StandardScaler())
# #             ]), safe_numeric))
# #         if safe_categorical:
# #             transformers.append(('cat', Pipeline([
# #                 ('imputer', SimpleImputer(strategy='most_frequent')),
# #                 ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
# #             ]), safe_categorical))
      
# #         preprocessor = ColumnTransformer(transformers, remainder='drop')
# #         task = "classification" if y.nunique() <= 10 else "regression"
      
# #         print(f"Task: {task} | Features: {len(safe_numeric)} numeric + {len(safe_categorical)} categorical")
      
# #         # SPLIT AFTER CLEANING
# #         X_train, X_test, y_train, y_test = train_test_split(
# #             X, y, test_size=0.2, random_state=42, 
# #             stratify=y if task == "classification" else None
# #         )
      
# #         # 3 MODEL BATTLE
# #         models = {
# #             "rf": RandomForestClassifier(n_estimators=100, random_state=42) if task == "classification" else RandomForestRegressor(n_estimators=100, random_state=42),
# #             "xgb": xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric="logloss") if task == "classification" else xgb.XGBRegressor(n_estimators=100, random_state=42),
# #             "lgb": LGBMClassifier(n_estimators=100, random_state=42, verbose=-1) if task == "classification" else LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
# #         }
      
# #         results = {}
# #         best_score = -np.inf
# #         best_pipeline = None
# #         best_model_name = None
      
# #         for name, model in models.items():
# #             print(f"Training {name}...")
# #             pipe = Pipeline([('preprocessor', preprocessor), ('model', model)])
# #             pipe.fit(X_train, y_train)
# #             y_pred = pipe.predict(X_test)
          
# #             # COMPLETE METRICS
# #             if task == "classification":
# #                 acc = accuracy_score(y_test, y_pred)
# #                 prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
# #                 metrics = {
# #                     'accuracy': float(acc), 'precision_macro': float(prec),
# #                     'recall_macro': float(rec), 'f1_macro': float(f1),
# #                     'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()
# #                 }
# #                 primary_score = acc
# #             else:
# #                 r2 = r2_score(y_test, y_pred)
# #                 mse = mean_squared_error(y_test, y_pred)
# #                 mae = mean_absolute_error(y_test, y_pred)
# #                 metrics = {'r2': float(r2), 'mse': float(mse), 'rmse': float(np.sqrt(mse)), 'mae': float(mae)}
# #                 primary_score = r2
          
# #             results[name] = {
# #                 'primary_score': float(primary_score),
# #                 'all_metrics': metrics,
# #                 'test_samples': len(X_test),
# #                 'train_samples': len(X_train),
# #                 'pred_samples': y_pred[:10].tolist(),
# #                 'true_vs_pred': list(zip(y_test.iloc[:10].tolist(), y_pred[:10].tolist()))
# #             }
          
# #             print(f"{name}: {primary_score:.4f}")
          
# #             if primary_score > best_score:
# #                 best_score = primary_score
# #                 best_pipeline = pipe
# #                 best_model_name = name
      
# #         # SAVE BEST MODEL + FULL RESULTS
# #         model_path = f"models/{dataset_id}.pkl"
# #         joblib.dump({
# #             'pipeline': best_pipeline, 'target': target_col, 'task': task,
# #             'best_model': best_model_name, 'full_results': results,
# #             'features': feature_cols, 'trained_on': data_source
# #         }, model_path)
      
# #         print(f"Saved best model: {best_model_name} ({best_score:.4f})")
      
# #         return safe_encoder({
# #             "status": "ok",
# #             "target": target_col,
# #             "task": task,
# #             "best_score": round(float(best_score), 4),
# #             "best_model": best_model_name.upper(),
# #             "model_leaderboard": results,
# #             "trained_on": data_source,
# #             "valid_rows": len(X),
# #             "message": f"üèÜ {best_model_name.upper()} WINS with {best_score:.4f} ({len(X)} valid rows)!"
# #         })
      
# #     except Exception as e:
# #         print(f"TRAIN ERROR: {str(e)}")
# #         print(traceback.format_exc())
# #         return safe_encoder({"status": "error", "message": f"Training failed: {str(e)[:150]}"})
# # @app.post("/predict/{dataset_id}")
# # def predict(dataset_id: str, data: dict):
# #     model_path = f"models/{dataset_id}.pkl"
# #     if not os.path.exists(model_path):
# #         return safe_encoder({"status": "error", "message": "Model not trained"})
  
# #     try:
# #         model_info = joblib.load(model_path)
# #         pipeline = model_info['pipeline']
# #         input_data = data.get("input_data", {})
# #         input_df = pd.DataFrame([input_data])
# #         prediction = pipeline.predict(input_df)[0]
      
# #         return safe_encoder({
# #             "status": "ok",
# #             "prediction": float(prediction),
# #             "target": model_info.get('target'),
# #             "task": model_info.get('task'),
# #             "model_used": model_info.get('best_model'),
# #             "confidence": "N/A"  # Add later
# #         })
# #     except Exception as e:
# #         return safe_encoder({"status": "error", "message": f"Prediction failed: {str(e)[:100]}"})
# # if __name__ == "__main__":
# #from fastapi import FastAPI, UploadFile, File, HTTPException

# # from fastapi import FastAPI, UploadFile, File, HTTPException
# # from fastapi.middleware.cors import CORSMiddleware
# # from fastapi.responses import FileResponse
# # from fastapi.encoders import jsonable_encoder

# # import os
# # import uuid
# # import json
# # import math
# # import time
# # import pandas as pd
# # import numpy as np
# # import joblib

# # from sklearn.pipeline import Pipeline
# # from sklearn.compose import ColumnTransformer
# # from sklearn.preprocessing import StandardScaler, OneHotEncoder
# # from sklearn.impute import SimpleImputer
# # from sklearn.model_selection import train_test_split
# # from sklearn.metrics import accuracy_score, r2_score
# # from sklearn.ensemble import (
# #     RandomForestClassifier,
# #     RandomForestRegressor,
# #     GradientBoostingClassifier,
# #     GradientBoostingRegressor,
# # )

# # # =====================================================
# # # APP
# # # =====================================================
# # app = FastAPI(title="OmniSearch AI ‚Äì Enterprise ML API")

# # app.add_middleware(
# #     CORSMiddleware,
# #     allow_origins=["*"],
# #     allow_methods=["*"],
# #     allow_headers=["*"],
# # )

# # # =====================================================
# # # DIRECTORIES
# # # =====================================================
# # BASE_DIR = os.getcwd()
# # DATA_DIR = os.path.join(BASE_DIR, "data")
# # DATASET_DIR = os.path.join(DATA_DIR, "datasets")
# # MODEL_DIR = os.path.join(BASE_DIR, "models")

# # os.makedirs(DATA_DIR, exist_ok=True)
# # os.makedirs(DATASET_DIR, exist_ok=True)
# # os.makedirs(MODEL_DIR, exist_ok=True)

# # # =====================================================
# # # SAFE JSON
# # # =====================================================
# # def safe(obj):
# #     return jsonable_encoder(
# #         obj,
# #         custom_encoder={
# #             float: lambda x: None if (math.isnan(x) or math.isinf(x)) else x,
# #             np.integer: int,
# #             np.floating: float,
# #             np.bool_: bool,
# #         },
# #     )

# # # =====================================================
# # # HELPERS
# # # =====================================================
# # def raw_path(dataset_id: str):
# #     return os.path.join(DATA_DIR, f"{dataset_id}.csv")

# # def dataset_dir(dataset_id: str):
# #     d = os.path.join(DATASET_DIR, dataset_id)
# #     os.makedirs(d, exist_ok=True)
# #     return d

# # def load_raw(dataset_id: str):
# #     p = raw_path(dataset_id)
# #     if not os.path.exists(p):
# #         raise HTTPException(404, "Raw dataset not found")
# #     return pd.read_csv(p)

# # def load_clean(dataset_id: str):
# #     p = os.path.join(dataset_dir(dataset_id), "clean.csv")
# #     if not os.path.exists(p):
# #         raise HTTPException(404, "Clean dataset not found")
# #     return pd.read_csv(p)

# # # =====================================================
# # # UPLOAD
# # # =====================================================
# # @app.post("/api/upload")
# # async def upload(file: UploadFile = File(...)):
# #     if not file.filename.lower().endswith(".csv"):
# #         raise HTTPException(400, "Only CSV files allowed")

# #     dataset_id = str(uuid.uuid4())[:8]
# #     path = raw_path(dataset_id)

# #     with open(path, "wb") as f:
# #         f.write(await file.read())

# #     try:
# #         preview = pd.read_csv(path, nrows=5)
# #     except Exception:
# #         raise HTTPException(400, "Invalid CSV")

# #     return safe({
# #         "status": "ok",
# #         "dataset_id": dataset_id,
# #         "columns": preview.columns.tolist(),
# #         "preview": preview.to_dict("records"),
# #     })

# # # =====================================================
# # # EDA
# # # =====================================================
# # @app.get("/api/eda/{dataset_id}")
# # def eda(dataset_id: str):
# #     try:
# #         df = load_clean(dataset_id)
# #         source = "CLEAN"
# #         etl_done = True
# #     except:
# #         df = load_raw(dataset_id)
# #         source = "RAW"
# #         etl_done = False

# #     missing = df.isna().sum()
# #     missing_pct = (missing.sum() / max(1, df.size)) * 100
# #     quality = max(0.0, 100.0 - missing_pct)

# #     return safe({
# #         "status": "ok",
# #         "eda": {
# #             "rows": int(len(df)),
# #             "columns": int(len(df.columns)),
# #             "missing": missing.to_dict(),
# #             "missing_pct": round(float(missing_pct), 2),
# #             "quality_score": round(float(quality), 1),
# #             "data_source": source,
# #             "etl_complete": etl_done,
# #             "summary": df.describe(include="all").to_dict(),
# #         }
# #     })

# # # =====================================================
# # # ETL
# # # =====================================================
# # @app.post("/api/datasets/{dataset_id}/clean")
# # def run_etl(dataset_id: str):
# #     df_raw = load_raw(dataset_id)
# #     df_clean = df_raw.copy()

# #     raw_rows = len(df_raw)
# #     raw_missing = int(df_raw.isna().sum().sum())
# #     raw_dupes = int(df_raw.duplicated().sum())

# #     # Drop duplicates
# #     df_clean = df_clean.drop_duplicates()

# #     # Fill missing values
# #     filled = 0
# #     for c in df_clean.select_dtypes(include="number"):
# #         before = df_clean[c].isna().sum()
# #         df_clean[c] = df_clean[c].fillna(df_clean[c].median())
# #         filled += int(before)

# #     for c in df_clean.select_dtypes(include="object"):
# #         before = df_clean[c].isna().sum()
# #         if not df_clean[c].mode().empty:
# #             df_clean[c] = df_clean[c].fillna(df_clean[c].mode()[0])
# #         filled += int(before)

# #     # Save clean dataset
# #     ddir = dataset_dir(dataset_id)
# #     clean_path = os.path.join(ddir, "clean.csv")
# #     df_clean.to_csv(clean_path, index=False)

# #     clean_rows = len(df_clean)
# #     clean_missing = int(df_clean.isna().sum().sum())

# #     raw_quality = max(0.0, 100.0 - (raw_missing / max(1, raw_rows)) * 100)
# #     clean_quality = max(0.0, 100.0 - (clean_missing / max(1, clean_rows)) * 100)

# #     accuracy_lift_expected = round(
# #         min(25.0, clean_quality - raw_quality), 2
# #     )

# #     comparison = {
# #         "raw_stats": {
# #             "rows": raw_rows,
# #             "missing_total": raw_missing,
# #             "duplicate_rows": raw_dupes,
# #         },
# #         "clean_stats": {
# #             "rows": clean_rows,
# #             "missing_total": clean_missing,
# #             "duplicate_rows": 0,
# #         },
# #         "improvements": {
# #             "missing_values_filled": filled,
# #             "duplicates_removed": raw_dupes,
# #             "outliers_fixed": 0,
# #         },
# #         "accuracy_lift_expected": accuracy_lift_expected,
# #     }

# #     with open(os.path.join(ddir, "comparison.json"), "w") as f:
# #         json.dump(comparison, f, indent=2)

# #     return safe({
# #         "status": "ok",
# #         "dataset_id": dataset_id,
# #         "comparison": comparison,
# #     })

# # # =====================================================
# # # DOWNLOADS
# # # =====================================================
# # @app.get("/api/datasets/{dataset_id}/download/raw")
# # def download_raw(dataset_id: str):
# #     p = raw_path(dataset_id)
# #     if not os.path.exists(p):
# #         raise HTTPException(404, "Raw file not found")
# #     return FileResponse(p, filename=f"{dataset_id}_raw.csv")

# # @app.get("/api/datasets/{dataset_id}/download/clean")
# # def download_clean(dataset_id: str):
# #     p = os.path.join(dataset_dir(dataset_id), "clean.csv")
# #     if not os.path.exists(p):
# #         raise HTTPException(404, "Run ETL first")
# #     return FileResponse(p, filename=f"{dataset_id}_clean.csv")

# # @app.get("/api/datasets/{dataset_id}/comparison")
# # def comparison(dataset_id: str):
# #     p = os.path.join(dataset_dir(dataset_id), "comparison.json")
# #     if not os.path.exists(p):
# #         raise HTTPException(404, "Run ETL first")
# #     with open(p) as f:
# #         return safe(json.load(f))

# # # =====================================================
# # # TRAIN
# # # =====================================================
# # @app.post("/api/train/{dataset_id}")
# # def train(dataset_id: str, payload: dict):
# #     target = payload.get("target")
# #     if not target:
# #         raise HTTPException(400, "Target column required")

# #     try:
# #         df = load_clean(dataset_id)
# #         source = "CLEAN"
# #     except:
# #         df = load_raw(dataset_id)
# #         source = "RAW"

# #     if target not in df.columns:
# #         raise HTTPException(400, "Target column not found")

# #     X = df.drop(columns=[target])
# #     y = df[target].dropna()
# #     X = X.loc[y.index]

# #     task = "classification" if y.nunique() <= 15 else "regression"

# #     num_cols = X.select_dtypes(include="number").columns
# #     cat_cols = X.select_dtypes(include="object").columns

# #     pre = ColumnTransformer([
# #         ("num", Pipeline([
# #             ("imp", SimpleImputer(strategy="median")),
# #             ("sc", StandardScaler())
# #         ]), num_cols),
# #         ("cat", Pipeline([
# #             ("imp", SimpleImputer(strategy="most_frequent")),
# #             ("oh", OneHotEncoder(handle_unknown="ignore"))
# #         ]), cat_cols),
# #     ])

# #     models = {
# #         "random_forest": RandomForestClassifier() if task == "classification" else RandomForestRegressor(),
# #         "gradient_boost": GradientBoostingClassifier() if task == "classification" else GradientBoostingRegressor(),
# #     }

# #     metric = accuracy_score if task == "classification" else r2_score

# #     Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)

# #     leaderboard = {}
# #     best_score = -1
# #     champion = None
# #     champion_name = None

# #     for name, model in models.items():
# #         pipe = Pipeline([("pre", pre), ("model", model)])
# #         pipe.fit(Xtr, ytr)
# #         score = metric(yte, pipe.predict(Xte))

# #         leaderboard[name] = {
# #             "primary_score": round(float(score), 4),
# #             "train_samples": len(Xtr),
# #             "test_samples": len(Xte),
# #         }

# #         if score > best_score:
# #             best_score = score
# #             champion = pipe
# #             champion_name = name

# #     model_root = os.path.join(MODEL_DIR, dataset_id)
# #     os.makedirs(model_root, exist_ok=True)

# #     version = f"v{len(os.listdir(model_root)) + 1}"
# #     vdir = os.path.join(model_root, version)
# #     os.makedirs(vdir, exist_ok=True)

# #     joblib.dump(champion, os.path.join(vdir, "model.pkl"))

# #     meta = {
# #         "status": "ok",
# #         "dataset_id": dataset_id,
# #         "version": version,
# #         "task": task,
# #         "target": target,
# #         "best_model": champion_name,
# #         "best_score": round(best_score, 4),
# #         "data_source": source,
# #         "leaderboard": leaderboard,
# #         "trained_at": time.strftime("%Y-%m-%d %H:%M:%S"),
# #     }

# #     with open(os.path.join(vdir, "metadata.json"), "w") as f:
# #         json.dump(meta, f, indent=2)

# #     return safe(meta)

# # # =====================================================
# # # META
# # # =====================================================
# # @app.get("/api/meta/{dataset_id}")
# # def meta(dataset_id: str):
# #     root = os.path.join(MODEL_DIR, dataset_id)
# #     if not os.path.exists(root):
# #         raise HTTPException(404, "Model not trained")

# #     latest = sorted(os.listdir(root))[-1]
# #     with open(os.path.join(root, latest, "metadata.json")) as f:
# #         return safe(json.load(f))

# # # =====================================================
# # # PREDICT
# # # =====================================================
# # @app.post("/api/predict/{dataset_id}")
# # def predict(dataset_id: str, payload: dict):
# #     root = os.path.join(MODEL_DIR, dataset_id)
# #     if not os.path.exists(root):
# #         raise HTTPException(404, "Model not trained")

# #     latest = sorted(os.listdir(root))[-1]
# #     model = joblib.load(os.path.join(root, latest, "model.pkl"))

# #     X = pd.DataFrame([payload])
# #     preds = model.predict(X)

# #     confidence = None
# #     if hasattr(model.named_steps["model"], "predict_proba"):
# #         try:
# #             confidence = float(model.predict_proba(X)[0].max())
# #         except Exception:
# #             confidence = None

# #     return safe({
# #         "prediction": preds.tolist(),
# #         "confidence": confidence,
# #         "model_version": latest,
# #     })
# # backend/app.py
# from fastapi import FastAPI, UploadFile, File, HTTPException
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import FileResponse
# from fastapi.encoders import jsonable_encoder

# import os, uuid, json, math, time
# import pandas as pd
# import numpy as np
# import joblib

# from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer
# from sklearn.preprocessing import StandardScaler, OneHotEncoder
# from sklearn.impute import SimpleImputer
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score, r2_score
# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

# # =====================================================
# # APP
# # =====================================================
# app = FastAPI(title="OmniSearch AI ‚Äì Enterprise ML API")

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # =====================================================
# # DIRECTORIES
# # =====================================================
# BASE = os.getcwd()
# DATA = os.path.join(BASE, "data")
# DATASETS = os.path.join(DATA, "datasets")
# MODELS = os.path.join(BASE, "models")

# os.makedirs(DATA, exist_ok=True)
# os.makedirs(DATASETS, exist_ok=True)
# os.makedirs(MODELS, exist_ok=True)

# # =====================================================
# # SAFE JSON
# # =====================================================
# def safe(obj):
#     return jsonable_encoder(
#         obj,
#         custom_encoder={
#             float: lambda x: None if (math.isnan(x) or math.isinf(x)) else x,
#             np.integer: int,
#             np.floating: float,
#             np.bool_: bool,
#         },
#     )

# # =====================================================
# # HELPERS
# # =====================================================
# def raw_path(dataset_id):
#     return os.path.join(DATA, f"{dataset_id}.csv")

# def dataset_dir(dataset_id):
#     d = os.path.join(DATASETS, dataset_id)
#     os.makedirs(d, exist_ok=True)
#     return d

# def load_raw(dataset_id):
#     p = raw_path(dataset_id)
#     if not os.path.exists(p):
#         raise HTTPException(404, "Dataset not found")
#     return pd.read_csv(p)

# # =====================================================
# # UPLOAD
# # =====================================================
# @app.post("/api/upload")
# async def upload(file: UploadFile = File(...)):
#     if not file.filename.lower().endswith(".csv"):
#         raise HTTPException(400, "Only CSV files allowed")

#     dataset_id = str(uuid.uuid4())[:8]
#     path = raw_path(dataset_id)

#     with open(path, "wb") as f:
#         f.write(await file.read())

#     preview = pd.read_csv(path, nrows=5)

#     return safe({
#         "status": "ok",
#         "dataset_id": dataset_id,
#         "columns": preview.columns.tolist(),
#         "preview": preview.to_dict("records"),
#     })

# # =====================================================
# # EDA
# # =====================================================
# @app.get("/api/eda/{dataset_id}")
# def eda(dataset_id: str):
#     df = load_raw(dataset_id)
#     missing = df.isna().sum()
#     missing_pct = (missing.sum() / max(1, df.size)) * 100

#     return safe({
#         "status": "ok",
#         "eda": {
#             "rows": int(len(df)),
#             "columns": int(len(df.columns)),
#             "missing": missing.to_dict(),
#             "missing_pct": round(float(missing_pct), 2),
#             "quality_score": round(100 - missing_pct, 1),
#             "data_source": "RAW",
#             "etl_complete": False,
#             "summary": df.describe(include="all").to_dict(),
#         }
#     })

# # =====================================================
# # ETL
# # =====================================================
# @app.post("/api/datasets/{dataset_id}/clean")
# def run_etl(dataset_id: str):
#     df = load_raw(dataset_id)
#     raw_rows = len(df)
#     raw_missing = int(df.isna().sum().sum())
#     raw_dupes = int(df.duplicated().sum())

#     df_clean = df.drop_duplicates().copy()

#     filled = 0
#     for c in df_clean.select_dtypes(include="number"):
#         n = df_clean[c].isna().sum()
#         df_clean[c] = df_clean[c].fillna(df_clean[c].median())
#         filled += int(n)

#     for c in df_clean.select_dtypes(include="object"):
#         n = df_clean[c].isna().sum()
#         if not df_clean[c].mode().empty:
#             df_clean[c] = df_clean[c].fillna(df_clean[c].mode()[0])
#         filled += int(n)

#     ddir = dataset_dir(dataset_id)
#     df_clean.to_csv(os.path.join(ddir, "clean.csv"), index=False)

#     comparison = {
#         "raw_stats": {
#             "rows": raw_rows,
#             "missing_total": raw_missing,
#             "duplicate_rows": raw_dupes,
#         },
#         "clean_stats": {
#             "rows": len(df_clean),
#             "missing_total": int(df_clean.isna().sum().sum()),
#             "duplicate_rows": 0,
#         },
#         "improvements": {
#             "missing_values_filled": filled,
#             "duplicates_removed": raw_dupes,
#         },
#     }

#     with open(os.path.join(ddir, "comparison.json"), "w") as f:
#         json.dump(comparison, f, indent=2)

#     return safe({
#         "status": "ok",
#         "dataset_id": dataset_id,
#         "comparison": comparison,
#     })

# # =====================================================
# # DOWNLOADS
# # =====================================================
# @app.get("/api/datasets/{dataset_id}/download/{kind}")
# def download(dataset_id: str, kind: str):
#     if kind == "raw":
#         p = raw_path(dataset_id)
#     elif kind == "clean":
#         p = os.path.join(dataset_dir(dataset_id), "clean.csv")
#     else:
#         raise HTTPException(400, "Invalid type")

#     if not os.path.exists(p):
#         raise HTTPException(404, "File not found")

#     return FileResponse(p, filename=os.path.basename(p))

# @app.get("/api/datasets/{dataset_id}/comparison")
# def comparison(dataset_id: str):
#     p = os.path.join(dataset_dir(dataset_id), "comparison.json")
#     if not os.path.exists(p):
#         raise HTTPException(404, "Run ETL first")
#     return safe(json.load(open(p)))

# # =====================================================
# # TRAIN
# # =====================================================
# @app.post("/api/train/{dataset_id}")
# def train(dataset_id: str, payload: dict):
#     target = payload.get("target")
#     if not target:
#         raise HTTPException(400, "Target required")

#     df = load_raw(dataset_id)
#     if target not in df.columns:
#         raise HTTPException(400, "Invalid target")

#     X = df.drop(columns=[target])
#     y = df[target].dropna()
#     X = X.loc[y.index]

#     task = "classification" if y.nunique() <= 15 else "regression"

#     num = X.select_dtypes(include="number").columns
#     cat = X.select_dtypes(include="object").columns

#     pre = ColumnTransformer([
#         ("num", Pipeline([
#             ("imp", SimpleImputer(strategy="median")),
#             ("sc", StandardScaler())
#         ]), num),
#         ("cat", Pipeline([
#             ("imp", SimpleImputer(strategy="most_frequent")),
#             ("oh", OneHotEncoder(handle_unknown="ignore"))
#         ]), cat),
#     ])

#     model = RandomForestClassifier() if task == "classification" else RandomForestRegressor()
#     pipe = Pipeline([("pre", pre), ("model", model)])

#     Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)
#     pipe.fit(Xtr, ytr)

#     score = accuracy_score(yte, pipe.predict(Xte)) if task == "classification" else r2_score(yte, pipe.predict(Xte))

#     root = os.path.join(MODELS, dataset_id)
#     os.makedirs(root, exist_ok=True)
#     version = f"v{len(os.listdir(root)) + 1}"
#     vdir = os.path.join(root, version)
#     os.makedirs(vdir)

#     joblib.dump(pipe, os.path.join(vdir, "model.pkl"))

#     meta = {
#         "status": "ok",
#         "dataset_id": dataset_id,
#         "version": version,
#         "task": task,
#         "target": target,
#         "best_model": "RandomForest",
#         "best_score": round(float(score), 4),
#         "leaderboard": {"RandomForest": round(float(score), 4)},
#         "trained_at": time.strftime("%Y-%m-%d %H:%M:%S"),
#     }

#     json.dump(meta, open(os.path.join(vdir, "metadata.json"), "w"), indent=2)
#     return safe(meta)

# # =====================================================
# # META
# # =====================================================
# @app.get("/api/meta/{dataset_id}")
# def meta(dataset_id: str):
#     root = os.path.join(MODELS, dataset_id)
#     if not os.path.exists(root):
#         raise HTTPException(404, "No model")
#     latest = sorted(os.listdir(root))[-1]
#     return safe(json.load(open(os.path.join(root, latest, "metadata.json"))))

# # =====================================================
# # PREDICT
# # =====================================================
# @app.post("/api/predict/{dataset_id}")
# def predict(dataset_id: str, payload: dict):
#     root = os.path.join(MODELS, dataset_id)
#     latest = sorted(os.listdir(root))[-1]
#     model = joblib.load(os.path.join(root, latest, "model.pkl"))

#     X = pd.DataFrame([payload])
#     preds = model.predict(X)

#     conf = None
#     if hasattr(model.named_steps["model"], "predict_proba"):
#         conf = float(model.predict_proba(X)[0].max())

#     return safe({
#         "prediction": preds.tolist(),
#         "confidence": conf,
#         "model_version": latest,
#     })

# backend/app.py
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.encoders import jsonable_encoder

import os, uuid, json, math, time
import pandas as pd
import numpy as np
import joblib

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error

from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import (
    RandomForestClassifier, RandomForestRegressor,
    GradientBoostingClassifier, GradientBoostingRegressor
)

# =====================================================
# APP
# =====================================================
app = FastAPI(title="OmniSearch AI ‚Äì Enterprise AutoML API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# =====================================================
# DIRECTORIES
# =====================================================
BASE = os.getcwd()
DATA = os.path.join(BASE, "data")
DATASETS = os.path.join(DATA, "datasets")
MODELS = os.path.join(BASE, "models")

os.makedirs(DATA, exist_ok=True)
os.makedirs(DATASETS, exist_ok=True)
os.makedirs(MODELS, exist_ok=True)

# =====================================================
# SAFE JSON
# =====================================================
def safe(obj):
    return jsonable_encoder(
        obj,
        custom_encoder={
            float: lambda x: None if (math.isnan(x) or math.isinf(x)) else x,
            np.integer: int,
            np.floating: float,
            np.bool_: bool,
        },
    )

# =====================================================
# HELPERS
# =====================================================
def raw_path(dataset_id):
    return os.path.join(DATA, f"{dataset_id}.csv")

def dataset_dir(dataset_id):
    d = os.path.join(DATASETS, dataset_id)
    os.makedirs(d, exist_ok=True)
    return d

def load_raw(dataset_id):
    p = raw_path(dataset_id)
    if not os.path.exists(p):
        raise HTTPException(404, "Dataset not found")
    return pd.read_csv(p)

# =====================================================
# UPLOAD
# =====================================================
@app.post("/api/upload")
async def upload(file: UploadFile = File(...)):
    if not file.filename.lower().endswith(".csv"):
        raise HTTPException(400, "Only CSV files allowed")

    dataset_id = str(uuid.uuid4())[:8]
    path = raw_path(dataset_id)

    with open(path, "wb") as f:
        f.write(await file.read())

    preview = pd.read_csv(path, nrows=5)

    return safe({
        "status": "ok",
        "dataset_id": dataset_id,
        "columns": preview.columns.tolist(),
        "preview": preview.to_dict("records"),
    })

# =====================================================
# EDA
# =====================================================
@app.get("/api/eda/{dataset_id}")
def eda(dataset_id: str):
    df = load_raw(dataset_id)
    missing = df.isna().sum()
    missing_pct = (missing.sum() / max(1, df.size)) * 100

    return safe({
        "status": "ok",
        "eda": {
            "rows": int(len(df)),
            "columns": int(len(df.columns)),
            "missing": missing.to_dict(),
            "missing_pct": round(float(missing_pct), 2),
            "quality_score": round(100 - missing_pct, 1),
            "data_source": "RAW",
            "etl_complete": False,
            "summary": df.describe(include="all").to_dict(),
        }
    })

# =====================================================
# ETL
# =====================================================
@app.post("/api/datasets/{dataset_id}/clean")
def run_etl(dataset_id: str):
    df = load_raw(dataset_id)

    raw_rows = len(df)
    raw_missing = int(df.isna().sum().sum())
    raw_dupes = int(df.duplicated().sum())

    df_clean = df.drop_duplicates().copy()

    filled = 0
    for c in df_clean.select_dtypes(include="number"):
        n = df_clean[c].isna().sum()
        df_clean[c] = df_clean[c].fillna(df_clean[c].median())
        filled += int(n)

    for c in df_clean.select_dtypes(include="object"):
        n = df_clean[c].isna().sum()
        if not df_clean[c].mode().empty:
            df_clean[c] = df_clean[c].fillna(df_clean[c].mode()[0])
        filled += int(n)

    ddir = dataset_dir(dataset_id)
    df_clean.to_csv(os.path.join(ddir, "clean.csv"), index=False)

    clean_missing = int(df_clean.isna().sum().sum())

    comparison = {
        "raw_stats": {
            "rows": raw_rows,
            "missing_total": raw_missing,
            "duplicate_rows": raw_dupes,
        },
        "clean_stats": {
            "rows": len(df_clean),
            "missing_total": clean_missing,
            "duplicate_rows": 0,
        },
        "improvements": {
            "missing_values_filled": filled,
            "duplicates_removed": raw_dupes,
            "outliers_fixed": 0
        },
        "accuracy_lift_expected": round(
            max(0.0, min(25.0, (raw_missing - clean_missing) / max(1, raw_rows) * 100)), 2
        )
    }

    with open(os.path.join(ddir, "comparison.json"), "w") as f:
        json.dump(comparison, f, indent=2)

    return safe({
        "status": "ok",
        "dataset_id": dataset_id,
        "comparison": comparison,
    })

# =====================================================
# DOWNLOADS
# =====================================================
@app.get("/api/datasets/{dataset_id}/download/{kind}")
def download(dataset_id: str, kind: str):
    if kind == "raw":
        p = raw_path(dataset_id)
    elif kind == "clean":
        p = os.path.join(dataset_dir(dataset_id), "clean.csv")
    else:
        raise HTTPException(400, "Invalid type")

    if not os.path.exists(p):
        raise HTTPException(404, "File not found")

    return FileResponse(p, filename=os.path.basename(p))

@app.get("/api/datasets/{dataset_id}/comparison")
def comparison(dataset_id: str):
    p = os.path.join(dataset_dir(dataset_id), "comparison.json")
    if not os.path.exists(p):
        raise HTTPException(404, "Run ETL first")
    return safe(json.load(open(p)))

# =====================================================
# TRAIN ‚Äî ENTERPRISE AUTOML
# =====================================================
@app.post("/api/train/{dataset_id}")
def train(dataset_id: str, payload: dict):
    target = payload.get("target")
    if not target:
        raise HTTPException(400, "Target required")

    df = load_raw(dataset_id)
    if target not in df.columns:
        raise HTTPException(400, "Invalid target")

    X = df.drop(columns=[target])
    y = df[target]
    mask = ~y.isna()
    X, y = X[mask], y[mask]

    if len(X) < 20:
        raise HTTPException(400, "Not enough valid rows")

    task = "classification" if y.nunique() <= 15 else "regression"

    num = X.select_dtypes(include="number").columns.tolist()
    cat = X.select_dtypes(include="object").columns.tolist()

    pre = ColumnTransformer([
        ("num", Pipeline([
            ("imp", SimpleImputer(strategy="median")),
            ("sc", StandardScaler())
        ]), num),
        ("cat", Pipeline([
            ("imp", SimpleImputer(strategy="most_frequent")),
            ("oh", OneHotEncoder(handle_unknown="ignore"))
        ]), cat),
    ])

    if task == "classification":
        models = {
            "LogisticRegression": LogisticRegression(max_iter=1000),
            "DecisionTree": DecisionTreeClassifier(),
            "RandomForest": RandomForestClassifier(n_estimators=200),
            "GradientBoosting": GradientBoostingClassifier(),
        }
        scorer = lambda yt, yp: accuracy_score(yt, yp)
    else:
        models = {
            "LinearRegression": LinearRegression(),
            "DecisionTree": DecisionTreeRegressor(),
            "RandomForest": RandomForestRegressor(n_estimators=200),
            "GradientBoosting": GradientBoostingRegressor(),
        }
        scorer = lambda yt, yp: r2_score(yt, yp)

    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)

    leaderboard = []
    best_score = -1e9
    best_model = None
    best_pipe = None

    for name, model in models.items():
        pipe = Pipeline([("pre", pre), ("model", model)])
        pipe.fit(Xtr, ytr)
        preds = pipe.predict(Xte)
        score = scorer(yte, preds)

        leaderboard.append({
            "model": name,
            "score": round(float(score), 4),
            "train_rows": len(Xtr),
            "test_rows": len(Xte)
        })

        if score > best_score:
            best_score = score
            best_model = name
            best_pipe = pipe

    root = os.path.join(MODELS, dataset_id)
    os.makedirs(root, exist_ok=True)
    joblib.dump(best_pipe, os.path.join(root, "model.pkl"))

    result = {
        "status": "ok",
        "task": task,
        "target": target,
        "best_model": best_model,
        "best_score": round(float(best_score), 4),
        "leaderboard": leaderboard,
        "trained_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "explanation": {
            "why_best": f"{best_model} achieved the highest validation score.",
            "why_not_others": "Other models underperformed on the same dataset and split."
        }
    }

    with open(os.path.join(root, "metadata.json"), "w") as f:
        json.dump(result, f, indent=2)

    return safe(result)

# =====================================================
# META
# =====================================================
@app.get("/api/meta/{dataset_id}")
def meta(dataset_id: str):
    p = os.path.join(MODELS, dataset_id, "metadata.json")
    if not os.path.exists(p):
        raise HTTPException(404, "No model trained")
    return safe(json.load(open(p)))

# =====================================================
# PREDICT
# =====================================================
@app.post("/api/predict/{dataset_id}")
def predict(dataset_id: str, payload: dict):
    model_path = os.path.join(MODELS, dataset_id, "model.pkl")
    if not os.path.exists(model_path):
        raise HTTPException(404, "Model not trained")

    model = joblib.load(model_path)
    X = pd.DataFrame([payload])
    preds = model.predict(X)

    conf = None
    if hasattr(model.named_steps["model"], "predict_proba"):
        conf = float(model.predict_proba(X)[0].max())

    return safe({
        "prediction": preds.tolist(),
        "confidence": conf,
    })